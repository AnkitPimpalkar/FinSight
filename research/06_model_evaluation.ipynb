{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b079a1d7",
   "metadata": {},
   "source": [
    "/config.yaml\n",
    "\n",
    "model_evaluation:\n",
    "  root_dir: artifacts/model_evaluation\n",
    "  metrics_file_name: metrics.json\n",
    "  model_path: artifacts/model_training/lstm_model.h5\n",
    "  X_test_path: artifacts/data_transformation/X_test.npy\n",
    "  y_test_path: artifacts/data_transformation/y_test.npy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09f8e049",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass(frozen=True)\n",
    "class ModelEvaluationConfig:\n",
    "    root_dir: Path\n",
    "    metrics_file_name: str\n",
    "    model_path: str\n",
    "    X_test_path: str\n",
    "    y_test_path: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfd3ab91",
   "metadata": {},
   "outputs": [],
   "source": [
    "from finance_ml.constants import *\n",
    "from finance_ml.utils.common import read_yaml, create_directories\n",
    "from finance_ml.entity.config_entity import DataIngestionConfig, DataTransformationConfig, ModelTrainingConfig, ModelTrainingParams, ModelEvaluationConfig \n",
    "import os\n",
    "\n",
    "class ConfigurationManager:\n",
    "    def __init__(\n",
    "        self,\n",
    "        config_filepath=CONFIG_FILE_PATH,\n",
    "        params_filepath=PARAMS_FILE_PATH,\n",
    "        schema_filepath=SCHEMA_FILE_PATH\n",
    "    ):\n",
    "        self.config = read_yaml(config_filepath)\n",
    "        self.params = read_yaml(params_filepath)\n",
    "        self.schema = read_yaml(schema_filepath)\n",
    "\n",
    "        create_directories([self.config.artifacts_root])\n",
    "\n",
    "    def get_model_evaluation_config(self) -> ModelEvaluationConfig:\n",
    "    config = self.config.model_evaluation\n",
    "\n",
    "    create_directories([config.root_dir])\n",
    "    \n",
    "    evaluation_config = ModelEvaluationConfig(\n",
    "        root_dir=config.root_dir,\n",
    "        metrics_file_name=config.metrics_file_name,\n",
    "        model_path=config.model_path,\n",
    "        X_test_path=config.X_test_path,\n",
    "        y_test_path=config.y_test_path\n",
    "    )\n",
    "\n",
    "    return evaluation_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c054cab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "import mlflow\n",
    "from pathlib import Path\n",
    "\n",
    "from finance_ml.entity.config_entity import ModelEvaluationConfig\n",
    "from finance_ml.utils.common import save_json\n",
    "\n",
    "class ModelEvaluation:\n",
    "    def __init__(self, config: ModelEvaluationConfig):\n",
    "        self.config = config\n",
    "\n",
    "    def evaluate(self):\n",
    "        # Use config paths for model and data\n",
    "        model_path = getattr(self.config, \"model_path\", os.path.join(\"artifacts\", \"model_training\", \"lstm_model.h5\"))\n",
    "        X_test_path = getattr(self.config, \"X_test_path\", os.path.join(\"artifacts\", \"data_transformation\", \"X_test.npy\"))\n",
    "        y_test_path = getattr(self.config, \"y_test_path\", os.path.join(\"artifacts\", \"data_transformation\", \"y_test.npy\"))\n",
    "\n",
    "        if not (os.path.exists(model_path) and os.path.exists(X_test_path) and os.path.exists(y_test_path)):\n",
    "            raise FileNotFoundError(\"Model or test data files not found. Please check the paths.\")\n",
    "\n",
    "        model = keras.models.load_model(model_path)\n",
    "        X_test = np.load(X_test_path)\n",
    "        y_test = np.load(y_test_path)\n",
    "\n",
    "        # Evaluate the model\n",
    "        results = model.evaluate(X_test, y_test, verbose=0)\n",
    "        if isinstance(results, list) and len(results) >= 2:\n",
    "            loss, rmse = results[:2]\n",
    "        else:\n",
    "            raise ValueError(\"Model evaluation did not return expected metrics (loss, rmse).\")\n",
    "\n",
    "        # Log metrics to MLflow\n",
    "        with mlflow.start_run():\n",
    "            mlflow.log_metric(\"test_loss_mae\", float(loss))\n",
    "            mlflow.log_metric(\"test_rmse\", float(rmse))\n",
    "            # Optionally log the model\n",
    "            mlflow.keras.log_model(model, \"model\")\n",
    "\n",
    "        # Save metrics to a JSON file\n",
    "        metrics = {\n",
    "            \"test_loss_mae\": float(loss),\n",
    "            \"test_rmse\": float(rmse)\n",
    "        }\n",
    "        metrics_file_path = Path(self.config.root_dir) / self.config.metrics_file_name\n",
    "        save_json(metrics_file_path, metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87e46377",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "from finance_ml.config.configuration import configurationManager\n",
    "from finance_ml.component.model_evaluation import ModelEvaluation\n",
    "from finance_ml import logger\n",
    "\n",
    "STAGE_NAME= \"Model Evaluation stage\"\n",
    "\n",
    "class ModelEvaluationPipeline:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def main(self):\n",
    "        try:\n",
    "            config_manager = ConfigurationManager()\n",
    "            evaluation_config = config_manager.get_model_evaluation_config()\n",
    "            \n",
    "            model_evaluation = ModelEvaluation(config = evaluation_config)\n",
    "            model_evaluation.evaluate()\n",
    "\n",
    "            logger.info (f\"{STAGE_NAME} Completed successfully\")\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.exception(e)\n",
    "            raise e\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    try:\n",
    "        logger.info(f'>>>>>>>>> stage {STAGE_NAME} started <<<<<<<<<')\n",
    "        obj = ModelEvaluationPipeline()\n",
    "        obj.main()\n",
    "        logger.info(f'>>>>>>>>> stage {STAGE_NAME} Completed <<<<<<<<<\\n\\nx==========x')\n",
    "    except Exception as e:\n",
    "        logger.exception(e)\n",
    "        raise e"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
